# TransformerReadList

## Description

Transformer readlist for my PL groupmates. Let us all jump to the rabbit hole of formally understanding transformers LOL.

## Introductory Material

1. https://arxiv.org/abs/1706.03762 Attention is all you need. This is the start of the story.
2. https://arxiv.org/abs/2207.09238 Nice description of the procedure of a transformer's computation.
3. https://nlp.seas.harvard.edu/2018/04/03/attention.html Implementation of a transformer described in 1.
4. https://amaarora.github.io/posts/2020-02-18-annotatedGPT2.html. "GPT" model. Basically GPT is just the decoder of the original transformer model. 


## More Advanced Discussions



## Papers on Formal Language and Neural Networks Reading List
### Neural Turing Machines
1. Classic Neural Turing Machine https://arxiv.org/abs/1410.5401
2. Review of existing implementations of turing machines. https://arxiv.org/pdf/1807.08518.pdf
3. Baby-NTM : https://arxiv.org/pdf/1911.03329.pdf
4. [Link](https://watermark.silverchair.com/neco_a_01060.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAtcwggLTBgkqhkiG9w0BBwagggLEMIICwAIBADCCArkGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMsb-uuEA5CSeDn-IUAgEQgIICivjmfIh4V6hNsINzujnPPYSZ_Ue3bFS0TWQn19thFpvx0tttsFAFRwZXaHNETHfiCcDn3J_rX9AQ-2QAY-TlqGTuuF6Ybn680Q8i6ErNFjO8TxGHEa9IQEC8UO0gIN3-YNp9Wfga7k3hNFbXoxc1KwARuM46NqzO4-IDMXWFxxKtXz_SPH0-Mx5zSCy0RMdvaeJBsgRW1CuMfDdgn5gN9DLD_psTl6agcCofxZbuOXiKcIie6a0mrlYtork1TpBecs-7y8rD24CAhtnczbgb3srwQS9e5rdjobVf4_PKa25wFH8arzC1ysNbxJVM9DbuvTsh_VVRjAW8b15hSBfV1uV7lr-mbgVBGowp-JV-qYNmr0XpUNU4DGPV-IWmIeQEqMd9h-K-geoOcl2gXYNkvGwPIt5t-FQfMmdg9_nx-f5njpHiMVZoz9gBrvBsLKrpdhRMk6TGLEhA6c-8cvnrxm0HmagON5ThHuAPWhskQINYkIe6WrsPvAd20bm82bALPPHak3R5L8HC1FFs7eRjp5qs3oPjZckj8LDKuYTuLtB1kt45QYmSLzzAH1273VYJM-XmGi6fXMDNgEYeZb958DcET5nZ2s8a9G_aY8uRVXXeva9iEpH_dYO3ef8L45U5p4yPWZDcyZBxVfMxn-cJCaSFk1ONxNnwwmf3eYZYYLHeKBgUDMBBV5epjiyFisPji_6uMFmWD5ARVgb5Wg2MmDpVjPeNLClyDdcPnlKnTIKENxxg-g7OaTbb_nkmncaiAJ2EPT16VljTgDttIDB4F3ccfvd4csl4YLphBDRIOFWK27WNSn-ELgyo0UjQfwTX_Lt98f4YjdQn8URl1dwidzcQk5Hl4jbNi5hQ)
5. Neural Compiler https://proceedings.neurips.cc/paper/2016/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf
__Remark: 'Neural Networks and Chomsky's Hierarchy' also includes implementations for some variants of NTM__
5. Neural Random Access Machines https://arxiv.org/pdf/1511.06392.pdf
6. Lie-Access NTM https://openreview.net/pdf?id=Byiy-Pqlx
7. Adaptive Neural Compilation https://proceedings.neurips.cc/paper/2016/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf

### Stack-RNN and its variants
1. Stack RNN: https://proceedings.neurips.cc/paper/2015/file/26657d5ff9020d2abefe558796b99584-Paper.pdf 
2. Multiple Memory-Augmented RNNs https://arxiv.org/pdf/1506.02516.pdf
3. Non-deterministic stack RNN: https://arxiv.org/abs/2010.04674
4. Weighted Stack RNN: https://arxiv.org/pdf/2210.06884.pdf
5. Neural Stacks: Context-Free Transductions with Neural Stacks https://arxiv.org/pdf/1809.02836.pdf
6. Neural Stacks: Finding Syntactic Representations in Neural Stacks https://arxiv.org/pdf/1906.01594.pdf
### Other works on formal language learning with NNs
1. Neural Networks and Chomsky's Hierarchy https://arxiv.org/abs/2207.02098
2. Analyzing Transformer's capabilities of recognizing formal languages https://arxiv.org/abs/2009.11264.pdf
3. Overcoming a Theoretical Limitation of Self-Attention https://arxiv.org/abs/2202.12172
4. Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity https://arxiv.org/pdf/2204.06618.pdf
5. On the Turing Completeness of Modern Neural Network Architectures: https://arxiv.org/abs/1901.03429
6. NEURAL-NETWORK GUIDED EXPRESSION TRANSFORMATION https://arxiv.org/pdf/1902.02194.pdf
7. Extracting Finite Automata from RNNs Using State Merging https://arxiv.org/pdf/2201.12451.pdf
8. Transformers Learn Shortcuts to Automata https://arxiv.org/pdf/2210.10749.pdf
9. Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions https://arxiv.org/pdf/2211.12316.pdf
10. BLOOM LARGE LANGUAGE MODELS AND THE CHOMSKY HIERARCHY https://openreview.net/pdf?id=jczReTpeJ0N
11. Grid-LSTMs https://arxiv.org/pdf/1507.01526.pdf
12. Survey of NN & FL https://arxiv.org/pdf/2006.01338.pdf
13. MAKING NEURAL PROGRAMMING ARCHITECTURES
GENERALIZE VIA RECURSION https://arxiv.org/pdf/1704.06611.pdf
### Graph representation of programs
1. https://openreview.net/pdf?id=iUGUc3HJ_3f
### Others
1. End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking https://arxiv.org/abs/2202.05826
2. What Can Neural Networks Reason About? https://arxiv.org/abs/1905.13211
### Transformers
https://arxiv.org/pdf/1704.06611.pdf
https://www.jmlr.org/papers/volume22/20-302/20-302.pdf
https://proceedings.neurips.cc/paper/2020/file/c8b9abffb45bf79a630fb613dcd23449-Paper.pdf - insights on limitations of transformers in learning subroutines
## Repos
### Stack NN's 
1. https://github.com/viking-sudo-rm/stacks
2. https://github.com/viking-sudo-rm/industrial-stacknns
3. https://github.com/viking-sudo-rm/stacknn-core
### NTMs
1. https://github.com/loudinthecloud/pytorch-ntm
2. 
## People and Blogs
1. Will Merrill https://lambdaviking.com/#about


## Analyzing Transformers
1. https://arxiv.org/abs/2301.13196
